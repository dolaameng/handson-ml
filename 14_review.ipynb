{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use tf.contrib.rnn.static_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_batch = np.array([\n",
    "    [[0, 1, 2], [9, 8, 7]],\n",
    "    [[3, 4, 5], [0, 0, 0]],\n",
    "    [[6, 7, 8], [6, 5, 4]],\n",
    "    [[9, 0, 1], [3, 2, 1]]\n",
    "], dtype=np.float32)\n",
    "\n",
    "seqlen_batch = np.array([2, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.94808394, -0.73269534],\n",
       "        [-1.        ,  0.32144389]],\n",
       "\n",
       "       [[-0.99998671, -0.87594861],\n",
       "        [ 0.49151343,  0.60313314]],\n",
       "\n",
       "       [[-1.        , -0.94487244],\n",
       "        [-0.99998677,  0.65276557]],\n",
       "\n",
       "       [[-0.76261401, -0.97092772],\n",
       "        [-0.96212488,  0.76977062]]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_outputs = 2\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "    X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]), axis=0)\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_outputs)\n",
    "    output_seqs, states = tf.contrib.rnn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)\n",
    "    outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2])\n",
    "\n",
    "    \n",
    "config = tf.ConfigProto(allow_soft_placement=True, \n",
    "                        log_device_placement=True)\n",
    "with tf.Session(config=config) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    outputs_val = outputs.eval(feed_dict={X: X_batch})\n",
    "    \n",
    "outputs_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use tf.nn.dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.7389456 ,  0.53873068],\n",
       "        [-1.        ,  0.99990129]],\n",
       "\n",
       "       [[-0.99997181,  0.98772043],\n",
       "        [ 0.        ,  0.        ]],\n",
       "\n",
       "       [[-1.        ,  0.99974543],\n",
       "        [-0.9999997 ,  0.99434185]],\n",
       "\n",
       "       [[-0.99998987,  0.98623681],\n",
       "        [-0.99672085,  0.75780749]]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "seq_len = tf.placeholder(tf.float32, [None])\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(n_outputs)\n",
    "y, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=np.float32, sequence_length=seq_len)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    y_val = y.eval(feed_dict={X: X_batch, seq_len: seqlen_batch})\n",
    "    \n",
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST as a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,), 255)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST Original\")\n",
    "mnist.data.shape, mnist.target.shape, mnist.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mnist(test_size=10000, batch_size=64):\n",
    "    \"\"\"return train_generator and test_data\n",
    "    \"\"\"\n",
    "    mnist = fetch_mldata(\"MNIST Original\")\n",
    "    X, y = (mnist.data / 255.).astype(np.float32), mnist.target.astype(np.float32)\n",
    "    X = X.reshape([-1, 28, 28])\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=test_size)\n",
    "    def _train_generator(train_X, train_y):\n",
    "        \n",
    "        train_X, train_y = shuffle(train_X, train_y)\n",
    "        i, n = 0, train_X.shape[0]\n",
    "        while True:\n",
    "            i %= n\n",
    "            yield train_X[i:i+batch_size], train_y[i:i+batch_size]\n",
    "            i += batch_size\n",
    "            \n",
    "    return _train_generator(train_X, train_y), (test_X, test_y)\n",
    "\n",
    "train_generator, (test_X, test_y) = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 28\n",
    "n_steps = 28\n",
    "batch_size = 64\n",
    "n_hiddens = 100\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "he_initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"rnn\"):\n",
    "    cell = tf.contrib.rnn.LSTMCell(n_hiddens, initializer=he_initializer)\n",
    "    seq_y, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
    "    seq_y_vec = seq_y[:,-1,:]\n",
    "    \n",
    "with tf.name_scope(\"softmax\"):\n",
    "    logits = tf.contrib.layers.fully_connected(seq_y_vec, 10, activation_fn=None)\n",
    "    \n",
    "with tf.name_scope(\"metrics\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    match = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(match, tf.float32))\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    labels = tf.argmax(logits, axis=1,)\n",
    "    \n",
    "with tf.name_scope(\"summary\"):\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    all_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"rnn-mnist\", graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2819 0.171875 0.1098\n",
      "0.344325 0.90625 0.9286\n",
      "0.205791 0.9375 0.957\n",
      "0.0559785 1.0 0.9633\n",
      "0.0948395 0.9375 0.9708\n",
      "0.0323619 1.0 0.9727\n",
      "0.0777591 0.984375 0.9756\n",
      "0.0205686 1.0 0.9787\n",
      "0.0664518 0.984375 0.9811\n",
      "0.0396016 0.984375 0.9806\n",
      "0.00645392 1.0 0.9815\n",
      "0.0141305 1.0 0.9803\n",
      "0.0133835 1.0 0.9795\n",
      "0.0174231 1.0 0.9806\n",
      "0.0427782 0.984375 0.9829\n",
      "0.0147423 1.0 0.9789\n",
      "0.0508408 0.984375 0.9844\n",
      "0.00365515 1.0 0.9841\n",
      "0.0109918 1.0 0.9859\n",
      "0.00428307 1.0 0.9853\n"
     ]
    }
   ],
   "source": [
    "n_epoches = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for e in range(n_epoches):\n",
    "        for b in range(60000 // batch_size):\n",
    "            X_batch, y_batch = next(train_generator)\n",
    "            _, summary = sess.run([train_op, all_summary], feed_dict={X: X_batch, y: y_batch})\n",
    "            if b % 500 == 0:\n",
    "                train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                                        feed_dict={X: X_batch, y: y_batch})\n",
    "                test_acc = sess.run(accuracy,\n",
    "                                   feed_dict={X: test_X, y: test_y})\n",
    "                print(train_loss, train_acc, test_acc)\n",
    "                writer.add_summary(summary=summary, global_step=e*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
