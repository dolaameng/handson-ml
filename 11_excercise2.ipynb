{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "from tensorflow.contrib.layers import dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load mnist, split into 2 parts\n",
    "- #1 for auxiliary training, same or not\n",
    "- #2 5000 images, traditional mnist classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    mnist = fetch_mldata(\"MNIST Original\")\n",
    "    # without normalizing input, bcs we expect to use batch normalization\n",
    "    X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int32)\n",
    "#     X /= 255 # not needed if batch norm is used\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch_generator(X, y, batch_size=64):\n",
    "    i, n = 0, X.shape[0]\n",
    "    while True:\n",
    "        i %= n\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        if i >= n: i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y = load_mnist()\n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for identical image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (1000, 784), (1000,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pair_batch_generator(X, y, batch_size=64):\n",
    "    idx = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        selected = np.random.choice(idx, batch_size)\n",
    "        x1 = X[selected]\n",
    "        x2 = []\n",
    "        for i, label in enumerate(y[selected]):\n",
    "            if i % 2 == 0:\n",
    "                choice = np.random.choice(np.where(y==label)[0], 1)\n",
    "            else:\n",
    "                choice = np.random.choice(np.where(y!=label)[0], 1)\n",
    "            x2.append(X[choice])\n",
    "        x2 = np.concatenate(x2)\n",
    "        yy = np.tile([0, 1], batch_size//2).astype(np.int32)\n",
    "        yield x1, x2, yy\n",
    "        \n",
    "pair_batches = pair_batch_generator(X1, y1)\n",
    "test_x1, test_x2, test_yy = next(pair_batch_generator(X1, y1, 1000))\n",
    "test_x1.shape, test_x2.shape, test_yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hiddens = [100] * 5\n",
    "batch_size = 64\n",
    "n_epoches = 20\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x1 = tf.placeholder(tf.float32, [None, n_inputs], name=\"x1\")\n",
    "x2 = tf.placeholder(tf.float32, [None, n_inputs], name=\"x2\")\n",
    "yy = tf.placeholder(tf.int32, [None], name=\"yy\")\n",
    "is_training = tf.placeholder(tf.bool, [], name=\"is_training\")\n",
    "keep_prob = 0.5\n",
    "\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.9,\n",
    "    \"updates_collections\": None,\n",
    "    \"scale\": True\n",
    "}\n",
    "he_init = variance_scaling_initializer()\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope([fully_connected],\n",
    "                  activation_fn=tf.nn.elu,\n",
    "                  normalizer_fn=batch_norm,\n",
    "                  normalizer_params=bn_params,\n",
    "                  weights_initializer=he_init):\n",
    "        prev1, prev2 = x1, x2\n",
    "        for i, n_hidden in enumerate(n_hiddens):\n",
    "            h1 = fully_connected(prev1, n_hidden,\n",
    "                                 scope=\"dnn1/hidden%i\"%i)\n",
    "            h1 = dropout(h1, keep_prob, is_training=is_training)\n",
    "            h2 = fully_connected(prev2, n_hidden,\n",
    "                                 scope=\"dnn2/hidden%i\"%i)\n",
    "            h2 = dropout(h2, keep_prob, is_training=is_training)\n",
    "            prev1, prev2 = h1, h2\n",
    "        h = tf.concat(1, [h1, h2])\n",
    "        hh = fully_connected(h, 100, scope=\"output-2\")\n",
    "        hh = dropout(hh, keep_prob, is_training=is_training)\n",
    "        logits = fully_connected(hh, 2, activation_fn=None, scope=\"output\")\n",
    "#         logits = tf.squeeze(logits, axis=1)\n",
    "        \n",
    "with tf.name_scope(\"loss\"):\n",
    "#     cast_yy = tf.cast(yy, tf.float32)\n",
    "#     xentropy = -cast_yy * tf.log(logits) - (1-cast_yy) * tf.log(1-logits)\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, yy)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "#     labels = tf.cast(logits >= 0.5, tf.int32)\n",
    "#     match = tf.equal(labels, yy)\n",
    "    match = tf.nn.in_top_k(logits, yy, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(match, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.400835 0.8125 0.874\n",
      "0.301539 0.921875 0.872\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-aaa71418e8a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./same_digit_model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epoches\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m55000\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mb_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_yy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_x1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_x2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb_yy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-4dc85e14d4e3>\u001b[0m in \u001b[0;36mpair_batch_generator\u001b[0;34m(X, y, batch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mchoice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:17201)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/dola/anaconda3/envs/py3tf/lib/python3.5/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   2431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2433\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2434\u001b[0m     \"\"\"\n\u001b[1;32m   2435\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproduct\u001b[0m \u001b[0mof\u001b[0m \u001b[0marray\u001b[0m \u001b[0melements\u001b[0m \u001b[0mover\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "#     init.run()\n",
    "    saver.restore(sess, \"./same_digit_model.ckpt\")\n",
    "    for i in range(n_epoches * 55000 // batch_size):\n",
    "        b_x1, b_x2, b_yy = next(pair_batches)\n",
    "        sess.run(train_op, feed_dict={x1: b_x1, x2: b_x2, yy: b_yy, is_training:True})\n",
    "        if i % 1000 == 0:\n",
    "            train_loss, train_acc = sess.run([loss, accuracy], \n",
    "                        feed_dict={x1: b_x1, x2: b_x2, yy: b_yy, is_training:False})\n",
    "            test_acc = sess.run(accuracy,\n",
    "                        feed_dict={x1: test_x1, x2: test_x2, yy: test_yy, is_training:False})\n",
    "            print(train_loss, train_acc, test_acc)\n",
    "    save_path = saver.save(sess, \"same_digit_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 dola dola 3.0M Feb 16 08:13 same_digit_model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 dola dola 4.8K Feb 16 08:13 same_digit_model.ckpt.index\r\n",
      "-rw-r--r-- 1 dola dola 1.3M Feb 16 08:13 same_digit_model.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh same_digit_model.ckpt*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### reconstruct dnn1 for mnist classification on dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4250, 784), (750, 784), (4250,), (750,))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size=0.15)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epoches = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = make_batch_generator(X_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the meta data into default graph\n",
    "tf.reset_default_graph()\n",
    "saver = tf.train.import_meta_graph(\"./same_digit_model.ckpt.meta\")\n",
    "default_graph = tf.get_default_graph()\n",
    "\n",
    "# reuse dnn1 and extend it\n",
    "n_outputs = 10\n",
    "# so now you can read the existing variables from saved graph\n",
    "X = default_graph.get_tensor_by_name(\"x1:0\")\n",
    "h = default_graph.get_tensor_by_name(\"dnn/dnn1/hidden4/Elu:0\")\n",
    "is_training = default_graph.get_tensor_by_name(\"is_training:0\")\n",
    "y = tf.placeholder(tf.int32, [None], \"y\")\n",
    "\n",
    "he_init = variance_scaling_initializer()\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"decay\": 0.9,\n",
    "    \"scale\": True,\n",
    "    \"updates_collections\": None\n",
    "}\n",
    "hh = fully_connected(X, 100, \n",
    "                         activation_fn=tf.nn.elu,\n",
    "                         normalizer_fn=batch_norm,\n",
    "                         normalizer_params=bn_params,\n",
    "                         weights_initializer=he_init, \n",
    "                         scope=\"mnist_hidden\")\n",
    "hh = dropout(hh, keep_prob=0.5, is_training=is_training, scope=\"mnist_dropout\")\n",
    "logits = fully_connected(hh, n_outputs, \n",
    "                         activation_fn=None,\n",
    "                         normalizer_fn=batch_norm,\n",
    "                         normalizer_params=bn_params,\n",
    "                         weights_initializer=he_init, \n",
    "                         scope=\"mnist_output\")\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"mnist*\")\n",
    "train_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "match = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(match, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mnist_hidden/weights:0',\n",
       " 'mnist_hidden/BatchNorm/beta:0',\n",
       " 'mnist_hidden/BatchNorm/gamma:0',\n",
       " 'mnist_output/weights:0',\n",
       " 'mnist_output/BatchNorm/beta:0',\n",
       " 'mnist_output/BatchNorm/gamma:0']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in train_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.42848 0.25 0.126667\n",
      "0.839103 0.875 0.869333\n",
      "0.71672 0.875 0.877333\n",
      "0.666933 0.9375 0.893333\n",
      "0.621029 0.9375 0.893333\n",
      "0.658727 0.96875 0.890667\n",
      "0.543276 0.90625 0.890667\n",
      "0.5338 0.9375 0.901333\n",
      "0.46006 0.9375 0.892\n",
      "0.428336 0.9375 0.893333\n",
      "0.540721 0.875 0.896\n",
      "0.397671 0.96875 0.905333\n",
      "0.680238 0.8125 0.908\n",
      "0.370433 0.9375 0.902667\n",
      "0.252974 0.96875 0.902667\n",
      "0.467164 0.90625 0.908\n",
      "0.312706 0.96875 0.906667\n",
      "0.294216 0.9375 0.902667\n",
      "0.308866 0.9375 0.904\n",
      "0.248386 0.96875 0.912\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run() # initialize other variables\n",
    "    saver.restore(sess, \"./same_digit_model.ckpt\") # load existing variables\n",
    "    for e in range(n_epoches):\n",
    "        for i in range(5000 // batch_size):\n",
    "            X_batch, y_batch = next(train_batches)\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch, is_training:True})\n",
    "            if i % 100 == 0:\n",
    "                train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                            feed_dict={X: X_batch, y: y_batch, is_training:False})\n",
    "                test_acc = sess.run(accuracy,\n",
    "                            feed_dict={X: X_test, y: y_test, is_training: False})\n",
    "                print(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
