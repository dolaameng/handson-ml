{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "from tensorflow.contrib.layers import dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data, split to 0 to 4 and 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    mnist = fetch_mldata(\"MNIST Original\")\n",
    "    # without normalizing input, bcs we expect to use batch normalization\n",
    "    X, y = mnist.data.astype(np.float32), mnist.target.astype(np.int32)\n",
    "    X, y = shuffle(X, y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = load_mnist()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35735, 784), (34265, 784), (35735,), (34265,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4, X5 = X[y<=4], X[y>=5]\n",
    "y4, y5 = y[y<=4], y[y>=5]-5\n",
    "X4.shape, X5.shape, y4.shape, y5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_batch_generator(X, y, batch_size=64):\n",
    "    i, n = 0, X.shape[0]\n",
    "    while True:\n",
    "        i %= n\n",
    "        yield X[i:i+batch_size], y[i:i+batch_size]\n",
    "        i += batch_size\n",
    "        if i >= n: i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a dnn for 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hiddens = [100] * 5\n",
    "n_outputs = 5\n",
    "batch_size = 64\n",
    "n_epoches = 20\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, [], name=\"is_training\")\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"updates_collections\": None,\n",
    "    \"decay\": 0.9,\n",
    "    \"scale\": True\n",
    "}\n",
    "he_init = variance_scaling_initializer()\n",
    "keep_prob = 0.5\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope([fully_connected],\n",
    "                  activation_fn=tf.nn.elu,\n",
    "                  weights_initializer=he_init,\n",
    "                  normalizer_fn=batch_norm,\n",
    "                  normalizer_params=bn_params):\n",
    "        prev = X\n",
    "        for i, n_hidden in enumerate(n_hiddens):\n",
    "            h = fully_connected(prev, n_hidden, scope=\"hidden%i\" % i)\n",
    "#             h = dropout(h, keep_prob, is_training=is_training)\n",
    "            prev = h\n",
    "    logits = fully_connected(h, n_outputs, activation_fn=None, scope=\"output\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    match = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(match, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden0/weights:0',\n",
       " 'hidden0/BatchNorm/beta:0',\n",
       " 'hidden0/BatchNorm/gamma:0',\n",
       " 'hidden0/BatchNorm/moving_mean:0',\n",
       " 'hidden0/BatchNorm/moving_variance:0',\n",
       " 'hidden1/weights:0',\n",
       " 'hidden1/BatchNorm/beta:0',\n",
       " 'hidden1/BatchNorm/gamma:0',\n",
       " 'hidden1/BatchNorm/moving_mean:0',\n",
       " 'hidden1/BatchNorm/moving_variance:0',\n",
       " 'hidden2/weights:0',\n",
       " 'hidden2/BatchNorm/beta:0',\n",
       " 'hidden2/BatchNorm/gamma:0',\n",
       " 'hidden2/BatchNorm/moving_mean:0',\n",
       " 'hidden2/BatchNorm/moving_variance:0',\n",
       " 'hidden3/weights:0',\n",
       " 'hidden3/BatchNorm/beta:0',\n",
       " 'hidden3/BatchNorm/gamma:0',\n",
       " 'hidden3/BatchNorm/moving_mean:0',\n",
       " 'hidden3/BatchNorm/moving_variance:0',\n",
       " 'hidden4/weights:0',\n",
       " 'hidden4/BatchNorm/beta:0',\n",
       " 'hidden4/BatchNorm/gamma:0',\n",
       " 'hidden4/BatchNorm/moving_mean:0',\n",
       " 'hidden4/BatchNorm/moving_variance:0',\n",
       " 'output/weights:0',\n",
       " 'output/biases:0',\n",
       " 'train/beta1_power:0',\n",
       " 'train/beta2_power:0',\n",
       " 'train/hidden0/weights/Adam:0',\n",
       " 'train/hidden0/weights/Adam_1:0',\n",
       " 'train/hidden0/BatchNorm/beta/Adam:0',\n",
       " 'train/hidden0/BatchNorm/beta/Adam_1:0',\n",
       " 'train/hidden0/BatchNorm/gamma/Adam:0',\n",
       " 'train/hidden0/BatchNorm/gamma/Adam_1:0',\n",
       " 'train/hidden1/weights/Adam:0',\n",
       " 'train/hidden1/weights/Adam_1:0',\n",
       " 'train/hidden1/BatchNorm/beta/Adam:0',\n",
       " 'train/hidden1/BatchNorm/beta/Adam_1:0',\n",
       " 'train/hidden1/BatchNorm/gamma/Adam:0',\n",
       " 'train/hidden1/BatchNorm/gamma/Adam_1:0',\n",
       " 'train/hidden2/weights/Adam:0',\n",
       " 'train/hidden2/weights/Adam_1:0',\n",
       " 'train/hidden2/BatchNorm/beta/Adam:0',\n",
       " 'train/hidden2/BatchNorm/beta/Adam_1:0',\n",
       " 'train/hidden2/BatchNorm/gamma/Adam:0',\n",
       " 'train/hidden2/BatchNorm/gamma/Adam_1:0',\n",
       " 'train/hidden3/weights/Adam:0',\n",
       " 'train/hidden3/weights/Adam_1:0',\n",
       " 'train/hidden3/BatchNorm/beta/Adam:0',\n",
       " 'train/hidden3/BatchNorm/beta/Adam_1:0',\n",
       " 'train/hidden3/BatchNorm/gamma/Adam:0',\n",
       " 'train/hidden3/BatchNorm/gamma/Adam_1:0',\n",
       " 'train/hidden4/weights/Adam:0',\n",
       " 'train/hidden4/weights/Adam_1:0',\n",
       " 'train/hidden4/BatchNorm/beta/Adam:0',\n",
       " 'train/hidden4/BatchNorm/beta/Adam_1:0',\n",
       " 'train/hidden4/BatchNorm/gamma/Adam:0',\n",
       " 'train/hidden4/BatchNorm/gamma/Adam_1:0',\n",
       " 'train/output/weights/Adam:0',\n",
       " 'train/output/weights/Adam_1:0',\n",
       " 'train/output/biases/Adam:0',\n",
       " 'train/output/biases/Adam_1:0']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30735, 784) (5000, 784) (30735,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X4, y4, test_size=5000, random_state=1337)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "train_batches = make_batch_generator(X_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9519 0.390625 0.3306\n",
      "0.0269714 0.984375 0.9832\n",
      "0.0133622 0.984375 0.9872\n",
      "0.0113916 1.0 0.988\n",
      "0.00499017 1.0 0.9894\n",
      "0.0150441 0.984375 0.9878\n",
      "0.00771244 1.0 0.9884\n",
      "0.00524801 1.0 0.986\n",
      "0.00231306 1.0 0.9856\n",
      "0.00125167 1.0 0.9858\n",
      "0.0197477 0.984375 0.989\n",
      "0.0129075 0.984375 0.99\n",
      "0.0029601 1.0 0.9874\n",
      "0.0151311 0.984375 0.9892\n",
      "0.0240409 0.984375 0.9886\n",
      "0.000307843 1.0 0.9892\n",
      "0.000259688 1.0 0.9902\n",
      "0.00123581 1.0 0.9902\n",
      "0.000252889 1.0 0.989\n",
      "0.0185084 0.984375 0.989\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for e in range(n_epoches):\n",
    "        for b in range(X_train.shape[0] // batch_size + 1):\n",
    "            X_batch, y_batch = next(train_batches)\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch, is_training: True})\n",
    "            if b % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                            feed_dict={X: X_batch, y: y_batch, is_training: False})\n",
    "                test_acc = sess.run(accuracy, feed_dict={X: X_test, y: y_test, is_training: False})\n",
    "                print(train_loss, train_acc, test_acc)\n",
    "        saver.save(sess, \"mnist0to4/model\", global_step=e)\n",
    "    save_path = saver.save(sess, \"mnist0to4/finalmodel.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnist0to4/finalmodel.ckpt'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transfer learning for 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hidden3/weights:0', 'hidden3/BatchNorm/beta:0', 'hidden3/BatchNorm/gamma:0', 'hidden4/weights:0', 'hidden4/BatchNorm/beta:0', 'hidden4/BatchNorm/gamma:0', 'output/weights:0', 'output/biases:0']\n"
     ]
    }
   ],
   "source": [
    "## recreate the graph\n",
    "n_inputs = 28 * 28\n",
    "n_hiddens = [100] * 5\n",
    "n_outputs = 5\n",
    "batch_size = 64\n",
    "n_epoches = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_inputs], name=\"X\")\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, [], name=\"is_training\")\n",
    "bn_params = {\n",
    "    \"is_training\": is_training,\n",
    "    \"updates_collections\": None,\n",
    "    \"decay\": 0.9,\n",
    "    \"scale\": True\n",
    "}\n",
    "he_init = variance_scaling_initializer()\n",
    "keep_prob = 0.5\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope([fully_connected],\n",
    "                  activation_fn=tf.nn.elu,\n",
    "                  weights_initializer=he_init,\n",
    "                  normalizer_fn=batch_norm,\n",
    "                  normalizer_params=bn_params):\n",
    "        prev = X\n",
    "        for i, n_hidden in enumerate(n_hiddens):\n",
    "            h = fully_connected(prev, n_hidden, scope=\"hidden%i\" % i)\n",
    "#             h = dropout(h, keep_prob, is_training=is_training)\n",
    "            prev = h\n",
    "    logits = fully_connected(h, n_outputs, activation_fn=None, scope=\"output\")\n",
    "    \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"output|hidden[34]\")\n",
    "    print([v.name for v in train_vars])\n",
    "    train_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    match = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(match, tf.float32))\n",
    "    \n",
    "\n",
    "saver = tf.train.Saver()\n",
    "# saver = tf.train.import_meta_graph(\"mnist0to4/finalmodel.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 784) (5000, 784) (1000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X5, y5, test_size=5000, random_state=1337)\n",
    "X_train, y_train = X_train[:1000], y_train[:1000] # for small dataset\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "train_batches = make_batch_generator(X_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.54107 0.3125 0.3768\n",
      "0.818634 0.828125 0.811\n",
      "0.365254 0.875 0.8494\n",
      "0.268277 0.890625 0.8722\n",
      "0.178556 0.953125 0.8876\n",
      "0.145689 0.96875 0.8978\n",
      "0.125442 0.984375 0.9078\n",
      "0.112284 0.984375 0.9094\n",
      "0.10106 0.96875 0.913\n",
      "0.0909153 0.96875 0.9152\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_path, )\n",
    "#     tf.global_variables_initializer().run()\n",
    "\n",
    "    for e in range(n_epoches):\n",
    "        for b in range(X_train.shape[0] // batch_size + 1):\n",
    "            X_batch, y_batch = next(train_batches)\n",
    "            sess.run(train_op, feed_dict={X: X_batch, y: y_batch, is_training: True})\n",
    "            if b % 1000 == 0:\n",
    "                train_loss, train_acc = sess.run([loss, accuracy],\n",
    "                            feed_dict={X: X_batch, y: y_batch, is_training: False})\n",
    "                test_acc = sess.run(accuracy, feed_dict={X: X_test, y: y_test, is_training: False})\n",
    "                print(train_loss, train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
